{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDCXP8at6us"
      },
      "source": [
        "\n",
        "A continuacion vamos a trabajar sobre un dataset que contiene datos sucios, al cual le vamos a aplicar tecnicas de limpieza como:\n",
        "\n",
        "‚úîÔ∏èConversion de tipos de datos<br>\n",
        "‚úîÔ∏èEliminacion de datos duplicados para evitar el doble conteo<br>\n",
        "‚úîÔ∏èRestrinccion de rangos para evitar datos futuros incoherentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "719dxmDl21d-"
      },
      "source": [
        "**üßæInformacion del Dataset**<br>\n",
        "El dataset que estaremos trabajando comprende informacion sobre las estaciones de inicio y finalizaci√≥n, la duraci√≥n del viaje y cierta informaci√≥n para el usuario de un servicio de bicicletas compartidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WaA6DmJlty0B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fnVlgvD90i3",
        "outputId": "2ffa86ba-8db4-40d3-aa40-2c6c62b6417d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\emilio.fernandez\\\\Desktop\\\\LimpiezaDatosPython\\\\ride_sharing_new.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Maestria Gaby\\Materia1\\LimpiezaDatosPython\\Emilio-LimpienzadeDatos.ipynb Celda 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Maestria%20Gaby/Materia1/LimpiezaDatosPython/Emilio-LimpienzadeDatos.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_ride \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39memilio.fernandez\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mDesktop\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mLimpiezaDatosPython\u001b[39;49m\u001b[39m\\\\\u001b[39;49;00m\u001b[39mride_sharing_new.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Maestria%20Gaby/Materia1/LimpiezaDatosPython/Emilio-LimpienzadeDatos.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#veamos que tiene el dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Maestria%20Gaby/Materia1/LimpiezaDatosPython/Emilio-LimpienzadeDatos.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df_ride\u001b[39m.\u001b[39minfo()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m     f,\n\u001b[0;32m   1218\u001b[0m     mode,\n\u001b[0;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1225\u001b[0m )\n\u001b[0;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\emilio.fernandez\\\\Desktop\\\\LimpiezaDatosPython\\\\ride_sharing_new.csv'"
          ]
        }
      ],
      "source": [
        "df_ride = pd.read_csv(\"C:\\\\Users\\\\emilio.fernandez\\\\Desktop\\\\LimpiezaDatosPython\\\\ride_sharing_new.csv\")\n",
        "#veamos que tiene el dataset\n",
        "df_ride.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vki4T4l94SmC"
      },
      "source": [
        "La columna `user_type` contiene informaci√≥n sobre si un usuario est√° viajando gratis y toma los siguientes valores:\n",
        "\n",
        "üö¥‚Äç‚ôÇÔ∏è `1` de viaje gratis <br>\n",
        "üö¥‚Äç‚ôÇÔ∏è `2` de pago por viaje <br>\n",
        "üö¥‚Äç‚ôÇÔ∏è `3` de suscriptores mensuales <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMZrkMJA07Ex",
        "outputId": "3a831df7-0298-48f0-93b2-623ff97759d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    76.000000\n",
              "mean      1.881579\n",
              "std       0.692187\n",
              "min       1.000000\n",
              "25%       1.000000\n",
              "50%       2.000000\n",
              "75%       2.000000\n",
              "max       3.000000\n",
              "Name: user_type, dtype: float64"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#utilizamos describe para ver el resumen estadistico de los datos de la columna user_type\n",
        "df_ride['user_type'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEj78tsU8RRO"
      },
      "source": [
        "üëÅÔ∏è‚Äçüó®Ô∏è Al observar las estadisticas resumidas de la columna `user_type` vemos que  tiene un conjunto finito de valores posibles que representan agrupaciones de datos, por ende, debemos convertir esta columna a tipo categor√≠a."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE0XLam39OJo",
        "outputId": "b386fa93-3ab7-40e3-e601-5832f7756608"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count     76\n",
            "unique     3\n",
            "top        2\n",
            "freq      39\n",
            "Name: user_type, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#podemos a cambiar el tipo de dato con astype, y se lo asignamos a la nueva columna\n",
        "df_ride['user_type'] = df_ride['user_type'].astype('category')\n",
        "\n",
        "#Para asegurarnos que la conversion fue correcta utilizamos la declaracion assert\n",
        "assert df_ride['user_type'].dtype == 'category'\n",
        "\n",
        "#y nuevamente observamos las estadisticas resumidas de la nueva columna, y podemos ver que\n",
        "#el resultado cambia al tratarse de una variable categorica\n",
        "print(df_ride['user_type'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSEvd6zj9rSp"
      },
      "source": [
        "üßπAhora bien, siguiendo con nuestra limpieza debemos evitar la inconsistencia de tipo de datos, es decir, debemos asesorarnos que los datos de tipo numericos sean numericos y asi sucesivamente con los demas datos. Esto se hace con el fin de que las operaciones matematicas que querramos realizar sobre esos datos arrojen resultados correctos y no erroneos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC16UVKFjoDF"
      },
      "source": [
        "Despues de haber revisado que contiene nuestro dataset, vamos a convertir la columna `durational` que es de tipo `object` a tipo `int`. Sin embargo, antes de eso, debemos asegurarnos de eliminar la palabra \"minutes\" para asi poderla convertir a numerica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Mn6XqIEiWxc",
        "outputId": "9f1a7c14-b8ec-45b5-ac89-77fcc10c9973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.776315789473685\n"
          ]
        }
      ],
      "source": [
        "#eliminamos la cadena minutes con el metodo strip \n",
        "df_ride['duration'] = df_ride['duration'].str.strip('minutes')\n",
        "\n",
        "#convertirmos duration a tipo int \n",
        "df_ride['duration'] = df_ride['duration'].astype('int')\n",
        "\n",
        "#Para asegurarnos que la conversion fue correcta utilizamos la declaracion assert\n",
        "assert df_ride['duration'].dtype == 'int'\n",
        "\n",
        "#Finalmente conocemos cual es la media de la columna duration_time\n",
        "print(df_ride['duration'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElMyAnGAmb3W"
      },
      "source": [
        "Ahora vamos a trabajar sobre la columna `tire_size` la cual contiene datos sobre el tama√±o de los neum√°ticos de cada bicicleta. el tama√±o de los neum√°ticos de las bicicletas pueden ser 26\" 27\" y 29\" y son almacenados con un valor float.\n",
        "\n",
        "Actualmente se quieren reducir los costos de mantenimiento, por consiguiente el proveedor decidi√≥ establecer el tama√±o m√°ximo de neum√°ticos en 27. Respecto a esto  vamos a asegurarnos de que la columna  `tire_sizes` tenga el rango correcto para esto establecemos el nuevo limite superior 27\" para los tama√±os de neum√°ticos, y despues convirtimos la columna a tipo categoria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZw1tN1coFlA",
        "outputId": "881c7fb4-ad96-4567-e5bc-4ee3a7ac9b29"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count     76\n",
              "unique     2\n",
              "top       27\n",
              "freq      73\n",
              "Name: tire_sizes, dtype: int64"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#usamos loc para seleccionar todo esos registros mayores a 27 establecerle el limite superior que es 27\n",
        "df_ride.loc[df_ride['tire_sizes'] > 27, 'tire_sizes'] = 27\n",
        "df_ride.head(10)\n",
        "\n",
        "#convetirmos a tipo categorico\n",
        "df_ride['tire_sizes'] = df_ride['tire_sizes'].astype('category')\n",
        "\n",
        "df_ride['tire_sizes'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NbMwkSMQiqz"
      },
      "source": [
        "üìÖ Ahora procederemos a encontrar todos los registros de la columna `ride_date` que tengan fechas futura, en caso de que si exista esta inconsistencia, lo que haremos sera establecer el valor maximo de esta columna con la fecha de hoy, pero antes de hacer eso debemos convetir a `ride_data` a tipo datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "La68bT9b3f2w",
        "outputId": "5ef86cd8-a742-47b5-83be-a39694d2663f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-04-05\n"
          ]
        }
      ],
      "source": [
        "#convertimos a ride_date en tipo de dato datetime\n",
        "infer_datetime_format=True\n",
        "df_ride['ride_date'] = pd.to_datetime(df_ride['ride_date']).dt.date\n",
        "\n",
        "#almacenamos la fecha de hoy\n",
        "today = dt.date.today()\n",
        "\n",
        "# y por ultimo le aplicamos la fecha de hoy almacenada en today a aquellos registros con\n",
        "#fechas futuras, es decir con fechas superiores a hoy\n",
        "df_ride.loc[df_ride['ride_date'] > today, 'ride_date'] = today\n",
        "\n",
        "#mostramos la fecha maxima\n",
        "print(df_ride['ride_date'].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7lhB-GQCMV9"
      },
      "source": [
        "LLego la hora, de encontrar los datos duplicados.\n",
        "Vamos a buscar que registros se encuentran duplicados en nuestro dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frqj6SM4BBDA",
        "outputId": "69ad60e3-fc12-45b5-e737-62cd793bd9fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    ride_id  duration  user_birth_year\n",
            "21       33        11             1986\n",
            "38       33         6             1966\n",
            "52       55        13             1999\n",
            "64       55        10             1999\n",
            "72       71        11             1987\n",
            "73       71        11             1987\n",
            "74       89         9             1990\n",
            "75       89         9             1990\n"
          ]
        }
      ],
      "source": [
        "# Aqui buscamos duplicados \n",
        "duplicates = df_ride.duplicated('ride_id', keep = False)\n",
        "\n",
        "# ordenamos por la columna ride_id los registros duplicados obtenidos\n",
        "df_ride = df_ride[duplicates].sort_values( by = 'ride_id')\n",
        "\n",
        "# y por ultimo imprimimos los datos duplicados, mostramos \n",
        "#en cuales columnas estaban estos registros\n",
        "print(df_ride[['ride_id','duration','user_birth_year']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYKNFUBLDXbR"
      },
      "source": [
        "Despues de conocer los duplicados y las columnas que los almacenan hemos podido observar que hay filas duplicadas tanto completas como incompletas para algunos valores de la columna `ride_id`, con valores diferentes para las columnas `user_birth_year` y duration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4kfeAYqGClH"
      },
      "source": [
        "Entonces, lo que vamos hacer es primero tratar esas filas duplicadas eliminando primero los duplicados completos y luego fusionando las filas duplicadas incompletas en una, manteniendo el promedio de `duration` y el m√≠nimo `user_birth_year` para cada conjunto de filas duplicadas incompletas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebVKpWAqBiSc"
      },
      "outputs": [],
      "source": [
        "# Eliminamos los duplicados completos en ride_sharing\n",
        "ride_dup = df_ride.drop_duplicates()\n",
        "\n",
        "# Creamos un diccionario de estadisticas para funciones de agregacion\n",
        "statistics = {'user_birth_year': 'min', 'duration': 'mean'}\n",
        "\n",
        "# Agrupamos por ride_id y calculamos las nuevas estadisticas\n",
        "ride_unique = ride_dup.groupby('ride_id').agg(statistics).reset_index()\n",
        "\n",
        "# nuevamente buscamos valores duplicados\n",
        "duplicates = ride_unique.duplicated(subset = 'ride_id', keep = False)\n",
        "duplicated_rides = ride_unique[duplicates == True]\n",
        "\n",
        "# y por ultimo nos aseguramos que se haya aplicado todo correctamente\n",
        "assert duplicated_rides.shape[0] == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhzEDfZ_WVfe"
      },
      "source": [
        "### **‚ùåüî°Problemas de texto y datos categ√≥ricos**\n",
        "\n",
        "En este espacio, vamos a corregir las incoherencias de hay con espacios en blanco y may√∫sculas en las etiquetas de las categor√≠as, tambien, vamos a contraer varias categor√≠as en una sola y cambiar el formato de las cadenas para mantener la coherencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vjk_DFmgLCV"
      },
      "source": [
        "**üßæInformacion del Dataset**<br>\n",
        "\n",
        "En este espacio trabajaremos con un dataset llamado `df_airlines` que contiene las respuestas de una encuesta que se le realizaron a los clientes de las aerolineas del aeropuerto de San Francisco. Este dataset tiene metadatos de vuelos, destinos, tiempos de espera, y tambien respuestas a preguntas de seguridad y sastifaccion\n",
        "\n",
        "Tambien trabajaremos con un dataset llamado `df_categories`, que contiene todos los valores posibles correctos para las columnas de la encuesta.\n",
        "\n",
        "‚úîÔ∏èUtilizaremos ambos dataset para encontrar respuestas de encuestas con valores inconsistentes y eliminarlas, con el fin de poder realizar de manera efectiva una combinaci√≥n externa e interna en ambos dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzRyz9LAfRfu",
        "outputId": "2b4c39a2-32d3-4273-fd4a-91243de345b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      cleanliness           safety          satisfaction\n",
            "0           Clean          Neutral     Very satisfaction\n",
            "1         Average        Very safe               Neutral\n",
            "2  Somewhat clean    Somewhat safe    Somewhat satisfied\n",
            "3  Somewhat dirty      Very unsafe  Somewhat unsatisfied\n",
            "4          Dirty   Somewhat unsafe      Very unsatisfied\n"
          ]
        }
      ],
      "source": [
        "df_categories = pd.read_csv('C:\\\\Users\\\\emilio.fernandez\\\\Desktop\\\\LimpiezaDatosPython\\\\categories.csv')\n",
        "df_airlines = pd.read_csv('C:\\\\Users\\\\emilio.fernandez\\\\Desktop\\\\LimpiezaDatosPython\\\\airlines_final.csv')\n",
        "\n",
        "#conozcamos cada una de las categorias\n",
        "print(df_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "os5m1T8Bh759",
        "outputId": "d4f6370d-eae0-4af8-b1c5-80821bf70e57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleanliness:  ['Clean' 'Average' 'Unacceptable' 'Somewhat clean' 'Somewhat dirty'\n",
            " 'Dirty'] \n",
            "\n",
            "Safety:  ['Neutral' 'Very safe' 'Somewhat safe' 'Very unsafe' 'Somewhat unsafe'] \n",
            "\n",
            "Satisfaction:  ['Very satisfied' 'Neutral' 'Somewhat satsified' 'Somewhat unsatisfied'\n",
            " 'Very unsatisfied'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Imprimimos los valores unicos de cada columna de la encuesta airlines\n",
        "#usando el metodo .unique()\n",
        "print('Cleanliness: ', df_airlines['cleanliness'].unique(), \"\\n\")\n",
        "print('Safety: ', df_airlines['safety'].unique(), \"\\n\")\n",
        "print('Satisfaction: ', df_airlines['satisfaction'].unique(), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV_gQqilpW8H"
      },
      "source": [
        "üëÅÔ∏è‚Äçüó®Ô∏è Despues de observar lo que contiene cada una de las categorias, hemos detectado que la columna `Cleanliness` de `df_airlines` tiene una categoria inconsistentes que es ***Unacceptable***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SawfNAB9q2pX"
      },
      "source": [
        "üë∑‚Äç‚ôÄÔ∏è Ahora vamos a crear un conjunto de la columna `cleanliness` en `df_airlines` y encontremos la categoria inconsistente encontrando la **diferencia** en la columna `cleanliness` de `df_categories`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCSazqiBmtKz",
        "outputId": "30130626-0664-4d96-b3e7-f69a57a8d453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        id        day          airline destination            dest_region  \\\n",
            "4     2992  Wednesday         AMERICAN       MIAMI                East US   \n",
            "586   2920   Thursday            DELTA     SEATTLE                West US   \n",
            "1816  2439  Wednesday  AIR NEW ZEALAND    AUCKLAND  Australia/New Zealand   \n",
            "\n",
            "     dest_size boarding_area   dept_time  wait_min   cleanliness     safety  \\\n",
            "4          Hub   Gates 50-59  2018-12-31     559.0  Unacceptable  Very safe   \n",
            "586        Hub   Gates 40-48  2018-12-31     180.0         Dirty    Neutral   \n",
            "1816    Medium  Gates 91-102  2018-12-31     250.0         Dirty    Neutral   \n",
            "\n",
            "            satisfaction  \n",
            "4     Somewhat satsified  \n",
            "586     Very unsatisfied  \n",
            "1816             Neutral  \n",
            "        id       day        airline        destination    dest_region  \\\n",
            "0     1351   Tuesday    UNITED INTL             KANSAI           Asia   \n",
            "1      373    Friday         ALASKA  SAN JOSE DEL CABO  Canada/Mexico   \n",
            "2     2820  Thursday          DELTA        LOS ANGELES        West US   \n",
            "3     1157   Tuesday      SOUTHWEST        LOS ANGELES        West US   \n",
            "5      634  Thursday         ALASKA             NEWARK        East US   \n",
            "...    ...       ...            ...                ...            ...   \n",
            "2804  1475   Tuesday         ALASKA       NEW YORK-JFK        East US   \n",
            "2805  2222  Thursday      SOUTHWEST            PHOENIX        West US   \n",
            "2806  2684    Friday         UNITED            ORLANDO        East US   \n",
            "2807  2549   Tuesday        JETBLUE         LONG BEACH        West US   \n",
            "2808  2162  Saturday  CHINA EASTERN            QINGDAO           Asia   \n",
            "\n",
            "     dest_size boarding_area   dept_time  wait_min     cleanliness  \\\n",
            "0          Hub  Gates 91-102  2018-12-31     115.0           Clean   \n",
            "1        Small   Gates 50-59  2018-12-31     135.0           Clean   \n",
            "2          Hub   Gates 40-48  2018-12-31      70.0         Average   \n",
            "3          Hub   Gates 20-39  2018-12-31     190.0           Clean   \n",
            "5          Hub   Gates 50-59  2018-12-31     140.0  Somewhat clean   \n",
            "...        ...           ...         ...       ...             ...   \n",
            "2804       Hub   Gates 50-59  2018-12-31     280.0  Somewhat clean   \n",
            "2805       Hub   Gates 20-39  2018-12-31     165.0           Clean   \n",
            "2806       Hub   Gates 70-90  2018-12-31      92.0           Clean   \n",
            "2807     Small    Gates 1-12  2018-12-31      95.0           Clean   \n",
            "2808     Large    Gates 1-12  2018-12-31     220.0           Clean   \n",
            "\n",
            "             safety        satisfaction  \n",
            "0           Neutral      Very satisfied  \n",
            "1         Very safe      Very satisfied  \n",
            "2     Somewhat safe             Neutral  \n",
            "3         Very safe  Somewhat satsified  \n",
            "5         Very safe      Very satisfied  \n",
            "...             ...                 ...  \n",
            "2804        Neutral  Somewhat satsified  \n",
            "2805      Very safe      Very satisfied  \n",
            "2806      Very safe      Very satisfied  \n",
            "2807  Somewhat safe      Very satisfied  \n",
            "2808      Very safe  Somewhat satsified  \n",
            "\n",
            "[2474 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "# Aplicamos la diferencia\n",
        "cat_clean = set(df_airlines['cleanliness']).difference(df_categories['cleanliness'])\n",
        "\n",
        "#Buscamos filas de df_airlines con un valor en cleanliness\n",
        "#que no este en df_categories\n",
        "cat_clean_rows = df_airlines['cleanliness'].isin(cat_clean)\n",
        "\n",
        "# Imprimimos las filas con categorias inconsistentes\n",
        "print(df_airlines[cat_clean_rows])\n",
        "\n",
        "# Imprimimos las filas solo con categorias consistentes\n",
        "print(df_airlines[~cat_clean_rows])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hvRidpqwt61"
      },
      "source": [
        "üí° Vamos a examinar las columnas categ√≥ricas `dest_region` y `dest_size` del dataset `df_airlines`, y nos aseguraremos de que est√©n limpias y listas para el an√°lisis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WADSgF7IxFMj",
        "outputId": "9d714460-7138-40a2-8720-4e8cfca8d9dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dest Region:  ['Asia' 'Canada/Mexico' 'West US' 'East US' 'Midwest US' 'EAST US'\n",
            " 'Middle East' 'Europe' 'eur' 'Central/South America'\n",
            " 'Australia/New Zealand' 'middle east'] \n",
            "\n",
            "Dest Size:  ['Hub' 'Small' '    Hub' 'Medium' 'Large' 'Hub     ' '    Small'\n",
            " 'Medium     ' '    Medium' 'Small     ' '    Large' 'Large     '] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Imprimimos los valores unicos de ambas columnas\n",
        "print('Dest Region: ', df_airlines['dest_region'].unique(), \"\\n\")\n",
        "print('Dest Size: ', df_airlines['dest_size'].unique(), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHfthIzjyFqY"
      },
      "source": [
        "Del resultado anterior podemos concluir los siguientes problemas:\n",
        "\n",
        "‚ö†Ô∏èLa columna `dest_region` tiene valores inconsistentes debido a las may√∫sculas y tiene un valor que debe reasignarse.\n",
        "\n",
        "‚ö†Ô∏èLa columna `dest_size` solo tiene valores inconsistentes debido a los espacios iniciales y finales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpcN61N5TG39"
      },
      "source": [
        "Para resolver los problemas que hemos dectetado procederemos a:<br>\n",
        "‚≠êCambiar las mayusculas de todos los valores de `dest_region` a minisculas.<br>\n",
        "‚≠êRemplazar `'eur'` con `'europe'` en `dest_region` usando el metodo `.replace()` <br>\n",
        "‚≠êQuitar los espacios en blancos de la columna `dest_size` usando el metodo `.strip()`<br>\n",
        "‚≠êVerificar que los cambios se hayan realizado con exito imprimiendo los valores unico de las columnas usando el metodo `.unique()`<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-T7WFRSzOPg",
        "outputId": "98621cd7-809e-48be-d305-324ec09d70ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dest Region:  ['asia' 'canada/mexico' 'west us' 'east us' 'midwest us' 'middle east'\n",
            " 'europe' 'central/south america' 'australia/new zealand'] \n",
            "\n",
            "Dest Size:  ['Hub' 'Small' 'Medium' 'Large'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_airlines['dest_region'] = df_airlines['dest_region'].str.lower()\n",
        "\n",
        "df_airlines['dest_region'] = df_airlines['dest_region'].replace({'eur':'europe'})\n",
        "\n",
        "df_airlines['dest_size'] = df_airlines['dest_size'].str.strip()\n",
        "\n",
        "print('Dest Region: ', df_airlines['dest_region'].unique(), \"\\n\")\n",
        "print('Dest Size: ', df_airlines['dest_size'].unique(), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgP79aOIXFh6"
      },
      "source": [
        "Para entender mejor a los encuestado, deseamos averiguar si existe una relaci√≥n entre ciertas respuestas y el d√≠a de la semana y el tiempo de espera en la puerta.\n",
        "\n",
        "el dataset `df_airlines` contiene las columnas `day` y `wait_min`, que son categ√≥ricas y num√©ricas respectivamente. La columna `day` contiene el d√≠a exacto en que se realiz√≥ un vuelo y `wait_min` la cantidad de minutos que los viajeros tardaron en esperar en la puerta de embarque. Para facilitar su an√°lisis, deseamos crear dos nuevas variables categ√≥ricas:\n",
        "\n",
        "`wait_type:` \n",
        "* `'short'` de 0 a 60 min \n",
        "* `'medium'`de 60 a 180 min \n",
        "* `'long'` mas de 180 min\n",
        "\n",
        "`day_week:`\n",
        "* `'weekday'`si el d√≠a est√° en el d√≠a de la semana\n",
        "* `'weekend'`si el d√≠a es en el fin de semana.\n",
        "\n",
        "Para resolverlo realizaremos los siguientes pasos:\n",
        "\n",
        "üëç Creamos los rangos y etiquetas para la columna `wait_type`<br>\n",
        "üëç Creamos la columna `wait_type` desde `wait_min` usando el metodo `pd.cut()`<br>\n",
        "üëç Creamos el diccionario `mapping` que ayudara a mapear a los dias de la semana `'weekday'` y a los dias del fin de semana`'weekend'`<br>\n",
        "Creamos la columna `day_week` usando `.replace()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suhssBgQYuZi"
      },
      "outputs": [],
      "source": [
        "# Creamos los rangos para categoria\n",
        "label_ranges = [0, 60, 180, np.inf]\n",
        "label_names = ['short', 'medium', 'long']\n",
        "\n",
        "# Creamos la columna wait_type  \n",
        "df_airlines['wait_type'] = pd.cut(df_airlines['wait_min'], bins =label_ranges, \n",
        "                                labels = label_names)\n",
        "\n",
        "# Creamos el diccionario\n",
        "mappings = {'Monday':'weekday', 'Tuesday':'weekday', 'Wednesday': 'weekday', \n",
        "            'Thursday': 'weekday', 'Friday': 'weekday', \n",
        "            'Saturday': 'weekend', 'Sunday': 'weekend'}\n",
        "\n",
        "df_airlines['day_week'] = df_airlines['day'].replace(mappings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "NsI8viqZbtPn",
        "outputId": "b66182b5-39d5-4d34-96ca-9460c3092bcb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>day</th>\n",
              "      <th>airline</th>\n",
              "      <th>destination</th>\n",
              "      <th>dest_region</th>\n",
              "      <th>dest_size</th>\n",
              "      <th>boarding_area</th>\n",
              "      <th>dept_time</th>\n",
              "      <th>wait_min</th>\n",
              "      <th>cleanliness</th>\n",
              "      <th>safety</th>\n",
              "      <th>satisfaction</th>\n",
              "      <th>wait_type</th>\n",
              "      <th>day_week</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1351</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>UNITED INTL</td>\n",
              "      <td>KANSAI</td>\n",
              "      <td>asia</td>\n",
              "      <td>Hub</td>\n",
              "      <td>Gates 91-102</td>\n",
              "      <td>2018-12-31</td>\n",
              "      <td>115.0</td>\n",
              "      <td>Clean</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Very satisfied</td>\n",
              "      <td>medium</td>\n",
              "      <td>weekday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>373</td>\n",
              "      <td>Friday</td>\n",
              "      <td>ALASKA</td>\n",
              "      <td>SAN JOSE DEL CABO</td>\n",
              "      <td>canada/mexico</td>\n",
              "      <td>Small</td>\n",
              "      <td>Gates 50-59</td>\n",
              "      <td>2018-12-31</td>\n",
              "      <td>135.0</td>\n",
              "      <td>Clean</td>\n",
              "      <td>Very safe</td>\n",
              "      <td>Very satisfied</td>\n",
              "      <td>medium</td>\n",
              "      <td>weekday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2820</td>\n",
              "      <td>Thursday</td>\n",
              "      <td>DELTA</td>\n",
              "      <td>LOS ANGELES</td>\n",
              "      <td>west us</td>\n",
              "      <td>Hub</td>\n",
              "      <td>Gates 40-48</td>\n",
              "      <td>2018-12-31</td>\n",
              "      <td>70.0</td>\n",
              "      <td>Average</td>\n",
              "      <td>Somewhat safe</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>medium</td>\n",
              "      <td>weekday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1157</td>\n",
              "      <td>Tuesday</td>\n",
              "      <td>SOUTHWEST</td>\n",
              "      <td>LOS ANGELES</td>\n",
              "      <td>west us</td>\n",
              "      <td>Hub</td>\n",
              "      <td>Gates 20-39</td>\n",
              "      <td>2018-12-31</td>\n",
              "      <td>190.0</td>\n",
              "      <td>Clean</td>\n",
              "      <td>Very safe</td>\n",
              "      <td>Somewhat satsified</td>\n",
              "      <td>long</td>\n",
              "      <td>weekday</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2992</td>\n",
              "      <td>Wednesday</td>\n",
              "      <td>AMERICAN</td>\n",
              "      <td>MIAMI</td>\n",
              "      <td>east us</td>\n",
              "      <td>Hub</td>\n",
              "      <td>Gates 50-59</td>\n",
              "      <td>2018-12-31</td>\n",
              "      <td>559.0</td>\n",
              "      <td>Unacceptable</td>\n",
              "      <td>Very safe</td>\n",
              "      <td>Somewhat satsified</td>\n",
              "      <td>long</td>\n",
              "      <td>weekday</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     id        day      airline        destination    dest_region dest_size  \\\n",
              "0  1351    Tuesday  UNITED INTL             KANSAI           asia       Hub   \n",
              "1   373     Friday       ALASKA  SAN JOSE DEL CABO  canada/mexico     Small   \n",
              "2  2820   Thursday        DELTA        LOS ANGELES        west us       Hub   \n",
              "3  1157    Tuesday    SOUTHWEST        LOS ANGELES        west us       Hub   \n",
              "4  2992  Wednesday     AMERICAN              MIAMI        east us       Hub   \n",
              "\n",
              "  boarding_area   dept_time  wait_min   cleanliness         safety  \\\n",
              "0  Gates 91-102  2018-12-31     115.0         Clean        Neutral   \n",
              "1   Gates 50-59  2018-12-31     135.0         Clean      Very safe   \n",
              "2   Gates 40-48  2018-12-31      70.0       Average  Somewhat safe   \n",
              "3   Gates 20-39  2018-12-31     190.0         Clean      Very safe   \n",
              "4   Gates 50-59  2018-12-31     559.0  Unacceptable      Very safe   \n",
              "\n",
              "         satisfaction wait_type day_week  \n",
              "0      Very satisfied    medium  weekday  \n",
              "1      Very satisfied    medium  weekday  \n",
              "2             Neutral    medium  weekday  \n",
              "3  Somewhat satsified      long  weekday  \n",
              "4  Somewhat satsified      long  weekday  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_airlines.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6F5hFyswfCw"
      },
      "source": [
        "Y finalmente nuestros datos han quedado limpios y listos para ser utilizados. üéâüéâüéâ"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN+vE3FVeYLsZoKwpePi8db",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1e8_Ocqw-PXiIQ5NB5yEhisSTztddXrF4",
      "name": "Cleaning_data_Python.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "2788042604e774a13887fb93638f8eb90673225097f3dc20f490326b20a49a42"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
